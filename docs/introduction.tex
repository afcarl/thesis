\chapter{introduction}
\doublespacenormalsize
Ambiguity in natural languages motivates researchers to use probabilistic models for natural language processing (NLP) tasks. Typically, a probabilistic model produces a posterior distribution over the space of all possible labels of an input. The probability of each label reflects the degree of uncertainty the model carries if it assigns that label to the input. The probabilistic predictions are then translated into final labels based on a decision-making scheme. Most of the time, performance of an NLP model is reported using metrics that only measure how well the predicted labels align with the true labels. The posterior distribution output by the model is, therefore, neglected.  

Nevertheless, in the context of NLP pipelines, knowing the quality of posterior predictions are extremely important in mitigating cascaded errors. Methods that uses K-best predictions \citep{sutton2005joint, wellner2004integrated, finkel2006solving} instead of using single best predictions weights the labels by their predicted probabilities. Although they were shown to give positive results, we suspect that the full potentials of these methods have not been recognized since they all assume that the tested models produce correct posterior predictions, which is not true in many cases. 

Besides that, knowledge about the uncertainty of predictions is useful for users of NLP systems. Reporting uncertainty of a model to its users is essential for risk assessment. Consider two predictions for a binary variable: one assigns a probability of 0.51 that the value of the variable is 1 and the other gives a probability of 0.99 for the same event. Using a threshold of 0.5, they will be both converted to positive predictions. The users are thus oblivious to the risk they are taking when trusting two predictions equally. On the other hand, if they knew the true uncertainties, we expect that they would trust the former prediction less than the latter one. This type of situation is very common for business users of NLP systems. Prediction confidence score are vital for them in calculating long-term revenue and making investment decisions.

In this thesis, we propose \textit{calibration analysis} as a framework for measuring the quality of the posterior predictions of probabilistic models. Calibration analysis is a powerful assessment tool for three reasons. First, perfect calibration is one of two main characteristics of perfect posterior predictions. Second, calibration analysis is simple to conduct and the results can be easily presented by a numerical score or a visualizable plot. Third, calibration analysis is extremely flexible. It is model-independent and application-independent. It only requires the tested model to output probabilistic predictions of an event of interest. The contributions of this work are three-fold: 

\begin{enumerate}
  \item Review the theoretical foundation of calibration analysis.
  \item Present two means of reporting results of the analysis: calibration scoring and calibration curve plotting. Describe adaptive binning as a simple nonparametric regression method for computing them. 
  \item Apply calibration analysis to investigating calibration patterns of models for POS tagging and coreference resolution. Especially, for coreference resolution, demonstrate a Monte Carlo sampling technique for approximate the probabilistic predictions, which are complicated to derive exactly.   
\end{enumerate}

