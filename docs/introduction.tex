\chapter{introduction}
\doublespacenormalsize
The ambiguity of natural languages motivates the use of the probabilistic models. Typically, a probabilistic model produces a posterior distribution over the space of all possible output labels for the input. The probability of an output label can be interpreted as the degree of belief that the model holds for the occurrence of that possibility. The model then makes its final decisions based on a decision-making scheme. In this setting, the posterior distribution output by the model is unknown to the users. In addition, performance metrics such as accuracy score or F-1 score also solely measure how to the final decisions align with the true labels and neglected the quality of the predicted posterior distribution. 

Many NLP systems use maximum-a-posteriori (MAP), i.e. choosing the most likely outcome, as their decision-making scheme. In complex NLP pipelines, however, MAP inference seems to be inappropriate since downstream models suffer from errors accumulated from upstream models. Using the K-top-most-likely predictions was suggested and shown to yield better performance for several NLP tasks \citep{sutton2005joint, wellner2004integrated}. \cite{finkel2006solving} proposed a more efficient alternative to K-best list, using Monte Carlo sampling to an approximate posterior distributions of upstream models and passed the samples as input for downstream models. Those approaches do not examine the quality of the posterior distributions and assume that they are reliable. This is problematic in practice since most NLP models are imperfect. As a result, the full potentials of those approaches would not have been recognized. 

Knowledge about the uncertainty of predictions is also useful for users of NLP systems. Communicating the uncertainty of a model with its users is essential for calculating risk. Consider two predictions for a binary variable: one gives a confidence score of 0.51 that the variable is 1 and the other assigns a score of 0.8 for the same event. Using a MAP decision-making scheme with a threshold of 0.5, the user will receive identical final predictions reporting that the value for the variable is 1 and will trust them equally. However, if they knew the true confidence scores, we would expect that they would trust the former predictions less than the latter one. This type of situation is very common for business users of NLP systems. Prediction confidence score are vital for them in calculating long-term revenue and making investment decisions.

In this thesis, we propose \textit{calibration analysis} as a procedure for measuring the quality of the posterior predictions of probabilistic models. Calibration analysis is a powerful assessment tool for three reasons. First, perfect calibration is one of two main conditions for being perfect at probabilistic prediction. Second, calibration analysis is simple to conduct and the results can be easily communicated to the users through a numerical score or a visualizable plot. Third, calibration analysis is extremely flexible. It is model-independent and application-independent. It can be applied to various types of output depending on the application of interest as long as we have a mechanism to obtain confidence scores from the model. The contributions of this work are: 

\begin{enumerate}
  \item Review the theoretical foundation for calibration analysis for NLP probabilistic models.
  \item Describe adaptive binning as a simple nonparametric regression method for calibration analysis. Also, present two means of reporting results of the analysis: calibration scoring and calibration curve plotting.
  \item Apply calibration analysis to investigating calibration patterns of models for POS tagging and coreference resolution. Especially, for coreference resolution, demonstrate a Monte Carlo sampling technique for approximate the confidence scores, which are complicated to compute exactly.   
\end{enumerate}

