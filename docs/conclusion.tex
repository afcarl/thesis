\chapter{Conclusions and implication for future research}

In this thesis, we have presented calibration analysis as a new framework for evaluating NLP probabilistic models. This framework directly measures the quality of posterior predictions rather than the quality of the final predictions. It is useful for investigating cascaded errors and estimating risks in NLP pipelines, whether the downstream tasks are stated explicitly or not. We propose two mechanisms to communicate the analysis results with the users, calibration score and calibration curve, and describe procedures to construct them. Since calibration analysis is not a common practice in NLP research, we choose to introduce fairly simple procedure so that researchers can implement for themselves. However, for this reason, approximations of the realistic frequencies involve errors. Hence, the confidence intervals constructed are also imperfect. In the future, we would like to reduce those errors by using more advanced non-parametric regression methods such as local likelihood, whose theoretical bounds have been well-studied. On the other hand, we would like apply our calibration analysis to obtain more insights into the behaviors of probabilistic models. In our experiments, discriminative models tend give better calibrated predictions than generative models. But does this fact also affect by the nature of data? Besides that, we would like to know if calibration of a type of query can infer calibration of other related types. Last but not least, a important question to ask was how calibration of an individual model would improve the overall performance of the entire pipeline. Answering this question requires setting up an end-to-end NLP pipeline and a study on recalibration methods.     
