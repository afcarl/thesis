\chapter{Report and discussion of research results}

\section{Calibration hyper-parameters}

Picking the optimal bin size can be done via cross-validation. However, since we have a fairly large amount of data points, we simply set the bin size to be 1000 for POS tagging, 400 for Tweet POS tagging and 10000 for coreference resolution. 

We report the MSE calibration score and its confidence intervals for all our experiments. To compute the score, we implement the sampling described in section 3.2.2 with a sample of size of 1000.



\section{Part-of-speech tagging}

\subsection{Data}

We extract WSJ articles from the CoNLL-2011 dataset for this experiment. The original data set has already been splitted the into training, development and testing sets so we filter WSJ articles from each set and join the sentences into together. This process results in 11772 sentences for training, 1632 sentences for development and 1382 sentences for testing. Running the models on the testing set produces 30543 prediction-observation pairs. We then conduct calibration analysis on this set of pairs. The query tested is whether a word has the ``NN'' tag. 

We train a HMM model using maximum likelihood principle. For CRF training, we implement the L2-regularized mini-batch AdaGrad method \citep{duchi2011adaptive}. The batch size is choosen by cross-validation.   

\subsection{Results}

\begin{figure}[t]
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{pos_hmm.jpg}
  \caption{}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{pos_crf.jpg}
  \caption{}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{pos_crf_full.jpg}
  \caption{}
\end{subfigure}
\caption{Calibration curves for HMM and CRF models (POS tagging) (a) HMM (b) CRF with basic features (c) CRF with advanced features} 
\end{figure}

\begin{figure}[t]
  \centering
  \footnotesize
  \begin{tabular*}{0.55\textwidth}{@{\extracolsep{\fill}} | c | c | c | }
    \hline
    Model & Accuracy (\%) & CalibMSE \\ 
    \hline
    HMM & 88.10 & 0.042 $\pm$ 5.27e-5 \\
    \hline
    CRF (Basic) & 93.02 & 0.021 $\pm$ 6.81e-5 \\
    \hline
    CRF (Advanced) & 96.08 & 0.018 $\pm$ 6.11e-5 \\
    \hline
  \end{tabular*}
\caption{Accuracy and calibration scores of the models for POS.}
\end{figure}
Firstly, we compare HMM with a CRF model with basic features (CRF-Basic). As mentioned in section 3.4.1, CRF-Basic contains only the transition features and the emission features. Figure 4.1 shows calibration curves of the two models. CRF-Basic attains a significantly lower MSE calibration score than HMM does (0.019 vs. 0.035). Moreover, it also produces more refined predictions than those of HMM, as seen from the distributions of points along the x-axis.

Firstly, we compare HMM with a CRF model with basic features (CRF-Basic). As mentioned in section 3.4.1, CRF-Basic contains only the transition features and the emission features. Figures 4.1(a) and 4.1(b) show calibration curves of the two models. CRF-Basic attains a significantly lower MSE calibration score than HMM does (Figure 4.4). Moreover, the CRF-Basic's calibration has less mid-range points that of HMM, which indicates that the model generates more refined predictions.

To measure the affect of features on calibration, we add advanced features such as surrounding words, word shape, word length, prefixes and suffixes to CRF-Basic. We discover that as we use better features, we obtain more refined and calibrated predictions. The better featurized CRF (CRF-Advanced), which achieves a 96\% accuraccy on the task, produces a calibration curve just slightly off the PCC. 

\section{Twitter part-of-speech tagging}
\subsection{Data}

\begin{figure}[t]
\begin{subfigure}{0.32 \textwidth}
  \centering
  \includegraphics[width=\linewidth]{pos_tweet_hmm.jpg}
  \caption{}
 \end{subfigure}
\begin{subfigure}{0.32 \textwidth}
  \centering
  \includegraphics[width=\linewidth]{pos_tweet_crf.jpg}
  \caption{}
\end{subfigure}
\begin{subfigure}{0.32 \textwidth}
  \centering
  \includegraphics[width=\linewidth]{pos_tweet_crf_full.jpg}
  \caption{}
\end{subfigure}
\caption{Calibration curves for HMM and CRF models (TweetPOS) (a) HMM (b) CRF with basic features. (c) CRF with advanced features} 
\end{figure}

\begin{figure}[t]
  \centering
  \footnotesize
  \begin{tabular*}{0.55\textwidth}{@{\extracolsep{\fill}} | c | c | c | }
    \hline
    Model & Accuracy (\%) & CalibMSE \\ 
    \hline
    HMM & 67.55 & 0.075 $\pm$ 1.40e-4 \\
    \hline
    CRF (Basic) & 78.12 & 0.034 $\pm$ 1.61e-4 \\
    \hline
    CRF (Advanced) & 86.19 & 0.048 $\pm$ 1.47e-4 \\
    \hline
  \end{tabular*}
\caption{Accuracy and calibration scores of the models for TweetPOS.}
\end{figure}


We repeat the same analysis in section 4.2 predicting POS tags for tweets. For this experiment, we use the ARK's Twitter POS data set \citep{gimpel2011part}, which consists of 1000 sentences for training, 327 sentences for development, 500 sentences for testing. The number of prediction-observation pairs obtain from testing set is 6160. Unlike the regular POS experiment, The query tested is whether a word has the ``V'' tag since we find that predicting ``N'' on tweets is still easy for the models.

\subsection{Results}

Comparing between HMM and CRF-Basic, we observe the same pattern as in the section 4.2. CRF-Basic gives a more calibrated and sharper predictions than HMM (Figure 4.4). In both experiments, the MSE calibration score of HMM is always twice as large as that of CRF-Basic. On the other hand, unlike the previous experiment, adding more features to CRF worsens the calibration score (Figure 4.4). While CRF-Advanced produces sharper predictions, it is overconfident at high-score predictions. There are two possible explanations for this phenomenon. First, the data size is smaller; hence, the score may be not approximated well enough. Second, the feature set increases the recall but not the precision of the model.

\section{Coreference resolution}

\subsection{Data}

For this experiment, we use the available implementation of the Bekerley coreference resolution (Berkeley-coref) \citep{durrett2013easy}. The data set used is the CoNLL-2011 data set. The performance score of this system was around 62\% on the development set. The query tested is whether two mentions belong to the same entity. We apply calibration analysis on about 4 millions predictions generated from the development set. 

\subsection{Results}

\begin{figure}[t]
\begin{subfigure}{0.4 \textwidth}
  \centering
  \includegraphics[width=\linewidth]{coref.jpg}
  \caption{}
 \end{subfigure}
\begin{subfigure}{0.6 \textwidth}
  \centering
  \footnotesize
  \begin{tabular*}{0.6\textwidth}{@{\extracolsep{\fill}} | c | c | }
    \hline
    Model & CalibMSE \\ 
    \hline
    Berkeley-coref & 0.007 $\pm$ 6.32e-6 \\
    \hline
  \end{tabular*}
  \caption{}
\end{subfigure}
\caption{Calibration analysis results for Berkeley-coref (a) Calibration curve  (b) Calibration score}
\end{figure}

Although having a far-from-perfect accuracy score, the system's predictions are extremely reliable (Figure). Its calibration score is even much smaller than the models in the POS task, which is a substantially easier task. The results also help explaining the effect of features on calibration. Berkeley-coref employs a carefully selected set of features called SURFACE. Whereas in the TweetPOS experiment, we see that adding features can degrade calibration, in this experiment, we observe a model with a good set of features can be very reliable despite making a lot of mistakes. Last but not least, throughout three experiements, it is not difficult to recognize that discriminative models, with a quality features, tend to be very well-calibrated.    
