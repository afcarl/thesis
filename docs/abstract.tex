\begin{abstract}
  We argue that measuring the reliability of probabilistic predictions is crucial for mitigating cascaded errors and estimating risks in natural language processing systems. Hence, we propose \textit{calibration analysis} as a new framework for assessing probabilistic predictions. Two methods are presented to report results of the analysis: calibration score and calibration curve. We show that they can be effectively estimated using a simple non-parametric regression method. Moreover, confidence intervals of the estimates can also be constructed. Finally, we apply calibration analysis on different families of probabilistic models to study the effects of model structure and feature on calibration.

\end{abstract}
