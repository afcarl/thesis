\begin{abstract}
  We argue that measuring the reliability of probabilistic predictions is crucial for mitigating cascaded errors and estimating risks in natural language processing systems. Hence, we propose \textit{calibration analysis} as a new tool for assessing the probabilistic predictions. Two methods are offered to present analysis results: calibration score and calibration curve. We show that they can be effectively estimated using a simple non-parametric regression method. Moreover, confidence intervals of the estimates can also be constructed. We conduct calibration on different families of probabilistic models and discovered patterns on the effects of model structure, sample size, and feature on calibration.

\end{abstract}
