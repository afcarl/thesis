\chapter{Summary of work of previous researchers}
\doublespacenormalsize

The notion of calibration was developed in the field of meteorology (Miller (1962), Murphy (1973)) and was referred to as validity or reliability. In Rubin (1982), it was argued that the applied statistician should Bayesian in principle and calibrated to the real world in practice. Murphy and Winkler (1984) proposed a general framework for forecast verification. They proposed two frameworks for analyzing posterior predictions, based on two ways to factorize the joint distribution of prediction and observation. We employ their calibration-refinement framework with a special focus on developing a procedure for measuring miscalibration of NLP models. \cite{degroot1983comparison} presented a well-defined theoretical scoring rule for comparing calibration and refinement of forecasters. However, since calculating the score requires knowledge of an unknown realistic distribution of the confidence scores, approximation methods from data sample must be used. In fact, this is a regression problem in which the data points are prediction-observation pairs. Parametric regression models are not flexible enough to generalize calibration patterns of different models. On the other hand, non-parametric models such as local regression \citep{wasserman2006all} not only gives better estimates but also provides good confidence intervals of the estimates. Adaptive binning is an easily implemented non-parametric method that is proved to be useful in biochemisty and computer vision \citep{davis2007adaptive, leow2004analysis}. Although we favor this method for the simplicity of our work, more advanced methods are available such as local likelihood \citep{frolich2006non}.  

On the NLP side, the problem of miscalibration in prediction models that employ single-best inference scheme was known for a long time \citep{draper1995assessment}. A Bayesian approach was proposed as an alternative. Finkel et. al (2009) apply this idea to tackle the problem of cascade NLP models. In this approach, the prediction posterior distribution of one task, which is approximated by a sample of the distribution, was passed as the input for other tasks. As suggested by the author, it is more general but easier to implement than approaches using K-best list (Sutton and McCallum (2005), Wellner et al. (2004), Huang and Chiang (2005), Toutanova et al. (2005)).


TODO: work on metrics for machine learning.

