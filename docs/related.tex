\chapter{Summary of work of previous researchers}
\doublespacenormalsize

Issues on neglecting uncertainty propagation in pipelines that employ single-best decision-making scheme were known for many years \citep{draper1995assessment}. Efforts to avoid a pipeline design in NLP were either increasing the model complexity \citep{singh2013joint, DurrettKlein2014} or ineffective \citep{sutton2005joint}. Besides that, approaches that preserve the pipeline structure and manage to mitigate cascaded errors such as modifying inference algorithms to obtain K best predictions gave useful improvements \citep{huang2005better, toutanova2005joint}. \cite{finkel2006solving} proposed a more clever way to obtain the top predictions using Monte Carlo inference. Unfortunately, in those approaches, the posterior predictions of the models are assumed to be reliable, which does not hold since the models are all imperfect. Performing calibration analysis on the models will be helpful to better understand the effectiveness of those approaches.

The notion of calibration was originally developed in the field of meteorology \citep{miller1962statistical, murphy1973new} and was referred to as validity or reliability. \cite{rubin1984bayesianly} argued that an applied statistician ``should be Bayesian in principle and calibrated to the real world in practice''. \cite{murphy1984probability} proposed a general framework for forecast verification. They proposed two frameworks for analyzing posterior predictions, based on two ways to factorize the joint distribution of prediction and observation. We employ their calibration-refinement framework with a special focus on developing a procedure for measuring miscalibration of NLP models. \cite{degroot1983comparison} presented a well-defined theoretical scoring rule for comparing calibration and refinement of forecasters. However, since calculating the score required knowledge of an unknown realistic distribution of the data, the score could not compute exactly in practice. Threfore, approximation methods from data sample must be used in this case. In fact, estimating realistic distribution is a regression problem where the data points are prediction-observation pairs. Parametric regression models are not flexible enough to generalize calibration patterns of different models. On the other hand, non-parametric models such as local regression \citep{wasserman2006all} not only gives better estimates but also provides confidence intervals of the estimates. Adaptive binning is an easily implemented non-parametric method that was proved to be useful in biochemisty and computer vision \citep{davis2007adaptive, leow2004analysis}. Although we favor this method for the purpose of simplicity, more advanced methods are available such as local likelihood \citep{frolich2006non}.  

Assessing uncertainty of probabilistic predictions is standard in many fields such as weather forecasting \citep{murphy1993good}, economics \citep{canova1994statistical, cooley1997calibrated} or earth sciences \citep{oreskes1994verification} but receives little attention in NLP. Calibration studies for general machine learning models \cite{niculescu2005predicting, caruana2006empirical} showed that applying recalibration techniques boosted performances of various models. Although these works also analyzed calibration and refinements of predictions through visualizable plots, they did not provide a concrete metric to quantify the degrees of miscalibration of the models. Our work not only provides more compact calibration plots but also describe an easy-to-implement method to calculate calibration score. Moreover, we also attempt to compute confidence intervals for all of our measurements.  




