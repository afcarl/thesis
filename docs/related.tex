\chapter{Summary of work of previous researchers}
\doublespacenormalsize

Issues on neglecting uncertainty propagation in pipelines that employ MAP scheme was known for many years \citep{draper1995assessment}. Efforts to abandon a pipeline architecture in NLP were either increasing the model complexity \citep{singh2013joint, DurrettKlein2014} or ineffective \citep{sutton2005joint}. Besides that, approaches that preserves the pipeline and manages to mitigate cascaded errors such as modifying inference algorithms to obtain K best predictions give useful improvements \citep{huang2005better, toutanova2005joint}. \cite{finkel2006solving} proposed a more clever way to obtain the top predictions using Monte Carlo inference. Unfortunately, the posterior predictions of the model are assumed to be reliable, which is not always the case for most NLP models. Therefore, calibration analysis will be helpful to better evaluate the effectiveness of those approaches.

The notion of calibration was originally developed in the field of meteorology (Miller (1962), Murphy (1973)) and was referred to as validity or reliability. In Rubin (1982), it was argued that the applied statistician should be Bayesian in principle and calibrated to the real world in practice. Murphy and Winkler (1984) proposed a general framework for forecast verification. They proposed two frameworks for analyzing posterior predictions, based on two ways to factorize the joint distribution of prediction and observation. We employ their calibration-refinement framework with a special focus on developing a procedure for measuring miscalibration of NLP models. \cite{degroot1983comparison} presented a well-defined theoretical scoring rule for comparing calibration and refinement of forecasters. However, since calculating the score requires knowledge of an unknown realistic distribution of the confidence scores, approximation methods from data sample must be used. In fact, this is a regression problem in which the data points are prediction-observation pairs. Parametric regression models are not flexible enough to generalize calibration patterns of different models. On the other hand, non-parametric models such as local regression \citep{wasserman2006all} not only gives better estimates but also provides good confidence intervals of the estimates. Adaptive binning is an easily implemented non-parametric method that is proved to be useful in biochemisty and computer vision \citep{davis2007adaptive, leow2004analysis}. Although we favor this method for the purpose of simplicity, more advanced methods are available such as local likelihood \citep{frolich2006non}.  

Assessing uncertainty of probabilistic predictions is standard in many fields such as weather forecasting \citep{murphy1993good}, economics \citep{canova1994statistical, cooley1997calibrated} or earth sciences \citep{oreskes1994verification} but receives little attention in NLP. Calibration studies for general machine learning models \cite{niculescu2005predicting, caruana2006empirical} showed that applying recalibration techniques boosted performances of various models. Although these works also analyzed calibration and refinements of predictions through visualizable plots, they did not provide a metric to quantify the degrees of miscalibration of the models. Our work not only provides more compact calibration plots but also describe a method to calculate calibration score. Moreover, we also attempt to compute confidence intervals for all of our measurements.  




